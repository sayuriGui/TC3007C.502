{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Actividad RNN\n",
        "## Tania Sayuri Guizado Hernandez A01640092"
      ],
      "metadata": {
        "id": "7xxiSE4ee3_I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4crpts1IUMyo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El libro que elegí fue Pride and Prejudice de Jane Austen"
      ],
      "metadata": {
        "id": "X-1q5VQnfVhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\"\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    book_text = response.text\n",
        "    print('Longitud del texto: {} caracteres'.format(len(book_text)))\n",
        "    unique_characters = sorted(set(book_text))\n",
        "    print ('El texto está compuesto de estos {} caracteres'.format(len(unique_characters)))\n",
        "    print (unique_characters)\n",
        "else:\n",
        "    print(\"Error al descargar el libro\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TG0N63j-UORh",
        "outputId": "3c9e43c9-2b83-496c-f24e-9f957bea745e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longitud del texto: 763062 caracteres\n",
            "El texto está compuesto de estos 100 caracteres\n",
            "['\\n', '\\r', ' ', '!', '#', '$', '%', '&', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}', '·', 'à', 'â', 'é', 'ê', 'œ', '—', '‘', '’', '“', '”', '•', '™', '\\ufeff']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tablas de traduccion o Inversa de vocabulario"
      ],
      "metadata": {
        "id": "d5_KkW4WVQ1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char2idx = {u:i for i, u in enumerate(unique_characters)}\n",
        "idx2char = np.array(unique_characters)\n",
        "\n",
        "for char,_ in zip(char2idx, range(len(unique_characters))):\n",
        "  print(' {:4s}: {:3d},'.format(repr(char), char2idx[char]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIn27sw_Ulee",
        "outputId": "d342a526-5a5e-4555-d47e-485c819b1004"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " '\\n':   0,\n",
            " '\\r':   1,\n",
            " ' ' :   2,\n",
            " '!' :   3,\n",
            " '#' :   4,\n",
            " '$' :   5,\n",
            " '%' :   6,\n",
            " '&' :   7,\n",
            " '(' :   8,\n",
            " ')' :   9,\n",
            " '*' :  10,\n",
            " ',' :  11,\n",
            " '-' :  12,\n",
            " '.' :  13,\n",
            " '/' :  14,\n",
            " '0' :  15,\n",
            " '1' :  16,\n",
            " '2' :  17,\n",
            " '3' :  18,\n",
            " '4' :  19,\n",
            " '5' :  20,\n",
            " '6' :  21,\n",
            " '7' :  22,\n",
            " '8' :  23,\n",
            " '9' :  24,\n",
            " ':' :  25,\n",
            " ';' :  26,\n",
            " '?' :  27,\n",
            " 'A' :  28,\n",
            " 'B' :  29,\n",
            " 'C' :  30,\n",
            " 'D' :  31,\n",
            " 'E' :  32,\n",
            " 'F' :  33,\n",
            " 'G' :  34,\n",
            " 'H' :  35,\n",
            " 'I' :  36,\n",
            " 'J' :  37,\n",
            " 'K' :  38,\n",
            " 'L' :  39,\n",
            " 'M' :  40,\n",
            " 'N' :  41,\n",
            " 'O' :  42,\n",
            " 'P' :  43,\n",
            " 'Q' :  44,\n",
            " 'R' :  45,\n",
            " 'S' :  46,\n",
            " 'T' :  47,\n",
            " 'U' :  48,\n",
            " 'V' :  49,\n",
            " 'W' :  50,\n",
            " 'X' :  51,\n",
            " 'Y' :  52,\n",
            " 'Z' :  53,\n",
            " '[' :  54,\n",
            " ']' :  55,\n",
            " '^' :  56,\n",
            " '_' :  57,\n",
            " 'a' :  58,\n",
            " 'b' :  59,\n",
            " 'c' :  60,\n",
            " 'd' :  61,\n",
            " 'e' :  62,\n",
            " 'f' :  63,\n",
            " 'g' :  64,\n",
            " 'h' :  65,\n",
            " 'i' :  66,\n",
            " 'j' :  67,\n",
            " 'k' :  68,\n",
            " 'l' :  69,\n",
            " 'm' :  70,\n",
            " 'n' :  71,\n",
            " 'o' :  72,\n",
            " 'p' :  73,\n",
            " 'q' :  74,\n",
            " 'r' :  75,\n",
            " 's' :  76,\n",
            " 't' :  77,\n",
            " 'u' :  78,\n",
            " 'v' :  79,\n",
            " 'w' :  80,\n",
            " 'x' :  81,\n",
            " 'y' :  82,\n",
            " 'z' :  83,\n",
            " '{' :  84,\n",
            " '}' :  85,\n",
            " '·' :  86,\n",
            " 'à' :  87,\n",
            " 'â' :  88,\n",
            " 'é' :  89,\n",
            " 'ê' :  90,\n",
            " 'œ' :  91,\n",
            " '—' :  92,\n",
            " '‘' :  93,\n",
            " '’' :  94,\n",
            " '“' :  95,\n",
            " '”' :  96,\n",
            " '•' :  97,\n",
            " '™' :  98,\n",
            " '\\ufeff':  99,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convertir texto a enteros"
      ],
      "metadata": {
        "id": "e8mwhn4SVKrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_as_int = np.array([char2idx[c] for c in book_text])\n",
        "\n",
        "#Mostramos algunos caracteres\n",
        "print('text: {}'.format(repr(book_text[:50])))\n",
        "print('{}'.format(repr(text_as_int[:50])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTtthgp5VIG9",
        "outputId": "37420adc-91f0-40a5-d11c-87a6421fb352"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: '\\ufeffThe Project Gutenberg eBook of Pride and Prejudic'\n",
            "array([99, 47, 65, 62,  2, 43, 75, 72, 67, 62, 60, 77,  2, 34, 78, 77, 62,\n",
            "       71, 59, 62, 75, 64,  2, 62, 29, 72, 72, 68,  2, 72, 63,  2, 43, 75,\n",
            "       66, 61, 62,  2, 58, 71, 61,  2, 43, 75, 62, 67, 78, 61, 66, 60])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparar datos"
      ],
      "metadata": {
        "id": "0Fkx60X5WbmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "seq_length = 100\n",
        "\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "AGeX4OicWYKG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#comprobar datos\n",
        "for item in sequences.take(10):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThSyoO8ZWijB",
        "outputId": "4902e457-c0ac-4dca-a54a-196c2969183e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'\\ufeffThe Project Gutenberg eBook of Pride and Prejudice\\r\\n    \\r\\nThis ebook is for the use of anyone anywhe'\n",
            "'re in the United States and\\r\\nmost other parts of the world at no cost and with almost no restrictions'\n",
            "'\\r\\nwhatsoever. You may copy it, give it away or re-use it under the terms\\r\\nof the Project Gutenberg Li'\n",
            "'cense included with this ebook or online\\r\\nat www.gutenberg.org. If you are not located in the United '\n",
            "'States,\\r\\nyou will have to check the laws of the country where you are located\\r\\nbefore using this eBoo'\n",
            "'k.\\r\\n\\r\\nTitle: Pride and Prejudice\\r\\n\\r\\n\\r\\nAuthor: Jane Austen\\r\\n\\r\\nRelease date: June 1, 1998 [eBook #1342]'\n",
            "'\\r\\n                Most recently updated: April 14, 2023\\r\\n\\r\\nLanguage: English\\r\\n\\r\\nCredits: Chuck Greif '\n",
            "'and the Online Distributed Proofreading Team at http://www.pgdp.net (This file was produced from imag'\n",
            "'es available at The Internet Archive)\\r\\n\\r\\n\\r\\n*** START OF THE PROJECT GUTENBERG EBOOK PRIDE AND PREJUDI'\n",
            "'CE ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                            [Illustration:\\r\\n\\r\\n                             GEORGE ALL'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparar datos de entrenamiento  (Entrada 0 a 99 )  (Salida 1 a 100)\n",
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "5quVPzKHWry4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizamos\n",
        "for input_example, target_example in dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data: ', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gtt7TgNW73T",
        "outputId": "0fa947aa-f089-4303-fadf-b184d6539b43"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data:  '\\ufeffThe Project Gutenberg eBook of Pride and Prejudice\\r\\n    \\r\\nThis ebook is for the use of anyone anywh'\n",
            "Target data:  'The Project Gutenberg eBook of Pride and Prejudice\\r\\n    \\r\\nThis ebook is for the use of anyone anywhe'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imprimir dataset\n",
        "print (dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtSb9555W926",
        "outputId": "3b1d1f7a-c02d-485c-9a56-5a3a76c8a67e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_MapDataset element_spec=(TensorSpec(shape=(100,), dtype=tf.int64, name=None), TensorSpec(shape=(100,), dtype=tf.int64, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#agrupar en batches\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJcIqadiXAtn",
        "outputId": "0f49dc4b-16f0-4dce-d7fa-178517d7cb7a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construir modelo RNN"
      ],
      "metadata": {
        "id": "qH6zV3qxXDed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                batch_input_shape=[batch_size,None]),\n",
        "\n",
        "      tf.keras.layers.LSTM(rnn_units,\n",
        "                           return_sequences=True,\n",
        "                           stateful = True,\n",
        "                           recurrent_initializer='glorot_uniform'),\n",
        "\n",
        "      tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "vocab_size = len(unique_characters)\n",
        "embedding_dim= 256\n",
        "rnn_units = 1024\n",
        "\n",
        "model = build_model(\n",
        "    vocab_size = vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size = BATCH_SIZE\n",
        ")"
      ],
      "metadata": {
        "id": "-ED_osLfXCwG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizar estructura\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPzm0etXXIib",
        "outputId": "4ecdaa2e-1862-4264-c9f9-3c57df744242"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           25600     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 100)           102500    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5375076 (20.50 MB)\n",
            "Trainable params: 5375076 (20.50 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Forma de input\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  print(\"Input: \", input_example_batch.shape, \"# (batch_size, lenght)\")\n",
        "  print(\"Target: \", target_example_batch.shape, \"# (batch_size, sequence_length)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_N6vLdwXI33",
        "outputId": "3ce7d39d-55f0-43ea-fe6a-c4f98f104c44"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  (64, 100) # (batch_size, lenght)\n",
            "Target:  (64, 100) # (batch_size, sequence_length)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Forma de salida\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(\"Prediction: \", example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KtrKtmtXR9t",
        "outputId": "7b38e1bd-f508-45a5-c252-c953c06374c1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction:  (64, 100, 100) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mostar que el resultado es una distribucion, no un argmax\n",
        "\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices_characters = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "print(sampled_indices_characters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07L8MX_wXTbI",
        "outputId": "f6086b61-e325-4c93-e609-e3242a027831"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[30 77 85 87 89 19 70 16 57 66 26 25 73  8 70 74 14 88 91 49 88 36  0 51\n",
            " 18 34 11 29 31 61 11 90 15 48 21 60 72 75 93 87  1 30 63  5 83 76 25 35\n",
            " 25  7  6 88 17 39 31 42 11 21 50 97 48 44 51 37 83 27 65 26 45 56 60 53\n",
            " 64 51 15 78 13 39 98 53 74 95 12 87 42  7 90 12 15 15 59 19 72 82 79 33\n",
            " 40 81 42 18]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento"
      ],
      "metadata": {
        "id": "jNSBVlfZXWuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "N1OqNU8lXVCT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_(epoch)\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")"
      ],
      "metadata": {
        "id": "9U5xaYPxXZvG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roYrGKqlXbJI",
        "outputId": "8b3b9355-413c-4b0b-c181-25db32687872"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "118/118 [==============================] - 13s 77ms/step - loss: 2.7623\n",
            "Epoch 2/50\n",
            "118/118 [==============================] - 9s 68ms/step - loss: 1.9958\n",
            "Epoch 3/50\n",
            "118/118 [==============================] - 10s 72ms/step - loss: 1.6499\n",
            "Epoch 4/50\n",
            "118/118 [==============================] - 11s 71ms/step - loss: 1.4430\n",
            "Epoch 5/50\n",
            "118/118 [==============================] - 10s 71ms/step - loss: 1.3251\n",
            "Epoch 6/50\n",
            "118/118 [==============================] - 10s 73ms/step - loss: 1.2512\n",
            "Epoch 7/50\n",
            "118/118 [==============================] - 10s 76ms/step - loss: 1.1986\n",
            "Epoch 8/50\n",
            "118/118 [==============================] - 11s 79ms/step - loss: 1.1564\n",
            "Epoch 9/50\n",
            "118/118 [==============================] - 12s 82ms/step - loss: 1.1210\n",
            "Epoch 10/50\n",
            "118/118 [==============================] - 12s 79ms/step - loss: 1.0879\n",
            "Epoch 11/50\n",
            "118/118 [==============================] - 10s 75ms/step - loss: 1.0600\n",
            "Epoch 12/50\n",
            "118/118 [==============================] - 10s 77ms/step - loss: 1.0304\n",
            "Epoch 13/50\n",
            "118/118 [==============================] - 11s 80ms/step - loss: 1.0023\n",
            "Epoch 14/50\n",
            "118/118 [==============================] - 11s 81ms/step - loss: 0.9739\n",
            "Epoch 15/50\n",
            "118/118 [==============================] - 10s 76ms/step - loss: 0.9435\n",
            "Epoch 16/50\n",
            "118/118 [==============================] - 10s 78ms/step - loss: 0.9140\n",
            "Epoch 17/50\n",
            "118/118 [==============================] - 11s 82ms/step - loss: 0.8841\n",
            "Epoch 18/50\n",
            "118/118 [==============================] - 10s 79ms/step - loss: 0.8530\n",
            "Epoch 19/50\n",
            "118/118 [==============================] - 11s 80ms/step - loss: 0.8203\n",
            "Epoch 20/50\n",
            "118/118 [==============================] - 10s 76ms/step - loss: 0.7892\n",
            "Epoch 21/50\n",
            "118/118 [==============================] - 10s 79ms/step - loss: 0.7553\n",
            "Epoch 22/50\n",
            "118/118 [==============================] - 11s 81ms/step - loss: 0.7237\n",
            "Epoch 23/50\n",
            "118/118 [==============================] - 11s 82ms/step - loss: 0.6946\n",
            "Epoch 24/50\n",
            "118/118 [==============================] - 12s 83ms/step - loss: 0.6642\n",
            "Epoch 25/50\n",
            "118/118 [==============================] - 12s 75ms/step - loss: 0.6354\n",
            "Epoch 26/50\n",
            "118/118 [==============================] - 11s 79ms/step - loss: 0.6091\n",
            "Epoch 27/50\n",
            "118/118 [==============================] - 12s 84ms/step - loss: 0.5818\n",
            "Epoch 28/50\n",
            "118/118 [==============================] - 11s 80ms/step - loss: 0.5578\n",
            "Epoch 29/50\n",
            "118/118 [==============================] - 11s 80ms/step - loss: 0.5340\n",
            "Epoch 30/50\n",
            "118/118 [==============================] - 11s 79ms/step - loss: 0.5142\n",
            "Epoch 31/50\n",
            "118/118 [==============================] - 11s 78ms/step - loss: 0.4956\n",
            "Epoch 32/50\n",
            "118/118 [==============================] - 11s 77ms/step - loss: 0.4772\n",
            "Epoch 33/50\n",
            "118/118 [==============================] - 11s 79ms/step - loss: 0.4615\n",
            "Epoch 34/50\n",
            "118/118 [==============================] - 11s 78ms/step - loss: 0.4450\n",
            "Epoch 35/50\n",
            "118/118 [==============================] - 10s 77ms/step - loss: 0.4317\n",
            "Epoch 36/50\n",
            "118/118 [==============================] - 11s 81ms/step - loss: 0.4202\n",
            "Epoch 37/50\n",
            "118/118 [==============================] - 12s 82ms/step - loss: 0.4088\n",
            "Epoch 38/50\n",
            "118/118 [==============================] - 10s 77ms/step - loss: 0.3998\n",
            "Epoch 39/50\n",
            "118/118 [==============================] - 10s 78ms/step - loss: 0.3920\n",
            "Epoch 40/50\n",
            "118/118 [==============================] - 11s 81ms/step - loss: 0.3839\n",
            "Epoch 41/50\n",
            "118/118 [==============================] - 12s 83ms/step - loss: 0.3741\n",
            "Epoch 42/50\n",
            "118/118 [==============================] - 11s 80ms/step - loss: 0.3682\n",
            "Epoch 43/50\n",
            "118/118 [==============================] - 10s 75ms/step - loss: 0.3619\n",
            "Epoch 44/50\n",
            "118/118 [==============================] - 10s 77ms/step - loss: 0.3555\n",
            "Epoch 45/50\n",
            "118/118 [==============================] - 11s 81ms/step - loss: 0.3519\n",
            "Epoch 46/50\n",
            "118/118 [==============================] - 11s 82ms/step - loss: 0.3461\n",
            "Epoch 47/50\n",
            "118/118 [==============================] - 11s 81ms/step - loss: 0.3417\n",
            "Epoch 48/50\n",
            "118/118 [==============================] - 11s 75ms/step - loss: 0.3375\n",
            "Epoch 49/50\n",
            "118/118 [==============================] - 10s 77ms/step - loss: 0.3345\n",
            "Epoch 50/50\n",
            "118/118 [==============================] - 11s 80ms/step - loss: 0.3306\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generación de texto"
      ],
      "metadata": {
        "id": "9-APCH-DXeNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1,None]))"
      ],
      "metadata": {
        "id": "Ym56WmrMXcj5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string, temp):\n",
        "  num_generate = 500\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  text_generated = []\n",
        "\n",
        "  temperature = temp\n",
        "\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "\n",
        "    predictions = tf.squeeze(predictions,0)\n",
        "\n",
        "    predictions = predictions/temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    input_eval = tf.expand_dims([predicted_id],0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return(start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "Lf2iHVvfXhC4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=u\"cat\", temp=0.5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JUWkNK2Xiqo",
        "outputId": "90c18d58-268e-432b-a583-5a1506554550"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat he had been obliged to\r\n",
            "got road. I know there are a great deal of good, I am sure I shall be extremely\r\n",
            "disappointed, she had been at first rather than he\r\n",
            "has discovered: I have seen them together for _her_, and his\r\n",
            "answering immediately to the last question, ‘Oh, the eldest daughter endeavoured to cllue her on his attention. He found her\r\n",
            "as handsome as she had been very great proper attention. The discourse of their brother’s admiration of Mr. Darcy, who think\r\n",
            "herself only the fair was an\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=u\"cat\", temp=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldVIf33Sgm_U",
        "outputId": "c50ef95f-1f8a-4040-f049-bc3a86481b21"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cathen they had just been down any\r\n",
            "access to a work with the phrase “I am determined not to mention\r\n",
            "Wickham. She would be incurted in astonishment was proved to. Mr. Bingley himself, ight parase, walked on, “is useprised?”\r\n",
            "\r\n",
            "“When Mr. Bennet hand, Colonel FOrstered?”\r\n",
            "\r\n",
            "“That is a particular daughter, if Mr. Bennet returned to Elizabeth.\r\n",
            "\r\n",
            "“I have known it only received from him which Lydia had for society, and even Sire\r\n",
            "regarded her now he production, nor in her former\r\n",
            "future behaviour, I be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=u\"cat\", temp=1.5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hS-_DC8rgqCo",
        "outputId": "3b5714dd-0bbe-45d9-c74c-323c65a8ba85"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat Fif your reachish_ has been\r\n",
            "origided. I caugh his need not so great enannonce! for I\r\n",
            "am in the hands you believe it done.”\r\n",
            "\r\n",
            "“On such exhing this room.\r\n",
            "\r\n",
            "He scynes:-ite suddenly intellecture soon. May\r\n",
            "cannothing, Mond his life was still in the good. In a do!”\r\n",
            "\r\n",
            "“Undeced ther. Our different\r\n",
            " in that. Never since does not one\r\n",
            "     dead Jane, I never saw sughes\r\n",
            "anxious to obsequise protesticable, since mare actually did I have they agreed. A\r\n",
            "     liked her, and despere is bust that he _“\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=u\"darcy\", temp=0.5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHc6fAL3XoLL",
        "outputId": "019ebede-8093-4466-c51d-2837224ae8f2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "darcy were at\r\n",
            "Rosings, and whenever he came to Hunsford; but without much success. He\r\n",
            "certainly was nothing; but he had still at intervals a kind\r\n",
            "listener in Mrs. Philips, was any comfortable one.\r\n",
            "Allest your sister had been at the\r\n",
            "hall; and in the interval of waiting appeared very long. It was over, they were obliged to go.\r\n",
            "Convinced her that nothing can be done; I\r\n",
            "will want to say that I have not time to express her admiration of Captain Carter, and\r\n",
            "when he complimented him on the other han\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=u\"darcy\", temp=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NU70KFVAgvs9",
        "outputId": "95834f31-1332-4cc1-b59f-0b902fe1df27"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "darcy with him service, the stabely in the\r\n",
            "world,” replied the other regard for the entail of painting of Mrs. Long and Bingley, that it was then in\r\n",
            "the house described in paradical, it is probable that he might scarcely givingly done. How I\r\n",
            "am persuaded she last saw her sway would amish in proposals, but it is,\r\n",
            "neit for her, was not lef happiness, openly acknowledged,--\r\n",
            "\r\n",
            "“When I so far such a very fine of its\r\n",
            "cool, and put Mr. Bennet should probably her husband, “and all the others the unhapp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=u\"darcy\", temp=1.5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GO060XmShDbK",
        "outputId": "5f8b942e-0527-4f2d-afdc-40478dca334c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "darcy certain\r\n",
            "1.I.” or of us consideration constrained\r\n",
            "preachise availmed, as her tapenn-to such dis_ over.”\r\n",
            "\r\n",
            "“I am have tolerably\r\n",
            "expressed what I did raturd you say there; as if she\r\n",
            "wished to offer him any attention, or remaint wifie, but had wonnet would have been most weld to\r\n",
            " eitieul is its rendered her friend, without loving its wilfelfialour, I\r\n",
            "shall hope to be tound. Collins,\r\n",
            "there were dishing to silence.\r\n",
            "\r\n",
            "“There _izzbushek to contain all not send them inst uponth?”\r\n",
            "\r\n",
            "“Yes; all o\n"
          ]
        }
      ]
    }
  ]
}